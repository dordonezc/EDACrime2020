{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code used to obtain dataset used in main notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that the file structure in the Path needs to be \n",
    "## CWD here X\n",
    "## 2020-01\n",
    "##       |------> 2020-01-city-of-london-street.csv\n",
    "##       |------> 2020-01-metropolitan-street.csv\n",
    "## 2020-02\n",
    "##       |------> 2020-02-city-of-london-street.csv\n",
    "##       |------> 2020-02-metropolitan-street.csv\n",
    "## etc.... \n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "os.chdir(\"C:/Users/dordo/Documents/Daniel/LSE/ST 445/Proyecto\")\n",
    "\n",
    "## Define function to clean monthly data\n",
    "def quick_clean(month, loc):\n",
    "    \"\"\" This function takes a month and a location as in the police data syntax\n",
    "    returns the dataset for that month in that location\"\"\"\n",
    "    path = \"2020-\" + month + \"/2020-\" + month + \"-\"  + loc + \"-street.csv\"\n",
    "    data = pd.read_csv(path)\n",
    "    ## Generate numeric ID \n",
    "    data[\"Id\"] = range(data.shape[0])\n",
    "    ## Generate Year/Month\n",
    "    data[['Year','Month']] = data.Month.str.split(\"-\",expand=True,)\n",
    "    ## Change Location\n",
    "    data['Location'] = data['Reported by'].str.replace(\"Police\",repl=\"\")\n",
    "    ## Keep only relevant variables\n",
    "    data.drop([\"Reported by\",\"Falls within\",\"Crime ID\", \"Context\", \"Year\"],\n",
    "              axis=1, inplace=True)\n",
    "    ## Drop data without location\n",
    "    data = data[~np.isnan(data.Latitude)]\n",
    "    return data\n",
    "\n",
    "## Define locations and months to be used\n",
    "locs = [\"city-of-london\", \"metropolitan\"]\n",
    "months = [\"0\" + str(num) for num in range(1,10)]\n",
    "iter_tup = tuple((month,loc) for month in months for loc in locs)\n",
    "\n",
    "## Load data and save to csv\n",
    "data_police = [quick_clean(*args) for args in iter_tup]\n",
    "final_data = pd.concat(data_police)\n",
    "final_data.to_csv(\"DataProject.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code used to obtain data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that the file structure in the Path needs to be \n",
    "## CWD here X\n",
    "## statistical-gis-boundaries-london\n",
    "##                                 |------> ESRI\n",
    "##                                           |----->MSOA_2011_London_gen_MHW.shp\n",
    "## Api\n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "import pickle\n",
    "os.chdir(\"C:/Users/dordo/Documents/Daniel/LSE/ST 445/Proyecto\")\n",
    "\n",
    "##--------------------------------------------------------------##\n",
    "## Load information on MSOA to use API \n",
    "data_MSOA = gpd.read_file(\"statistical-gis-boundaries-london/ESRI/MSOA_2011_London_gen_MHW.shp\")\n",
    "data_MSOA = data_MSOA.to_crs(epsg=4326)\n",
    "\n",
    "##---------------------------------------------------------------------------##\n",
    "## Functions used to make Api request: \n",
    "def get_coords(x):\n",
    "    \"\"\" Takes a polygon/multipolygon as given by a geometry\n",
    "    and returns the coordinate sequence\"\"\"\n",
    "    try:\n",
    "        coords = [item.exterior.coords for item in x]\n",
    "    except:\n",
    "        coords = [x.exterior.coords]\n",
    "    return coords\n",
    "\n",
    "def get_frisk_data(point, date, max_val = 3500):\n",
    "    \"\"\" Takes a polygon coordinates as parsed with the get_coords function\n",
    "    and a date return a count of frisks made in the polygon for that date\n",
    "    according to age\"\"\"\n",
    "    base = \"https://data.police.uk/api/stops-street?poly={}&date={}\"\n",
    "    poly_coords = \":\".join([str(val[1]) + \",\" + str(val[0]) for val in list(point)])\n",
    "    response = requests.get(base.format(poly_coords[:max_val], date))\n",
    "    content = response.json()\n",
    "    comp = [item[\"age_range\"] for item in content]\n",
    "    counts = pd.Categorical(comp, categories = [\"10-17\", \"18-24\", \"25-34\", \"over 34\"]).value_counts()\n",
    "    return counts\n",
    "\n",
    "def frisk_to_pandas(date, save_list, index_list):\n",
    "    \"\"\" Takes a date a list with MSOA polygons|Multipolygons along with the \n",
    "    index list to represent all the polygons in one of the multypoligons one. \n",
    "    Returns pandas dataframe for the frisk data\n",
    "    and pickle file is generated in the path with the corresponding month.\"\"\" \n",
    "    ## Request api data\n",
    "    serie = pd.Series()\n",
    "    for index, x in enumerate(save_list):\n",
    "        serie = pd.concat([serie, get_frisk_data(x, date)])\n",
    "    \n",
    "    ## Create data with MSOA index\n",
    "    fix_index = []\n",
    "    for val in index_list:\n",
    "        fix_index.extend([val] * 4)\n",
    "    fix_index = pd.DataFrame({\"MSOA ID\": fix_index})\n",
    "    \n",
    "    ## Merge Frisk data with MSOA \n",
    "    my_df = pd.concat([serie.reset_index(),fix_index], axis=1)\n",
    "    final_df = my_df.pivot(index=\"MSOA ID\", columns=\"index\", values=0)\n",
    "    final_df.columns = pd.Index(list(final_df.columns))\n",
    "    \n",
    "    ## Create ID for multipolygons in order to collapse\n",
    "    p = re.compile(\"[0-9]+\")\n",
    "    split = [(x,p.match(x).group()) for x in final_df.index]\n",
    "    match_tab = pd.DataFrame({\"Group\":[val[1] for val in split],\n",
    "                              \"MSOA ID\": [val[0] for val in split]})\n",
    "    final_df.reset_index(inplace=True)\n",
    "    \n",
    "    ## Collapse by Polygon/Multipolygon\n",
    "    final_dfm = pd.merge(final_df, match_tab).groupby(\"Group\").agg(sum)\n",
    "    \n",
    "    ## Paste with MSOA names\n",
    "    MSOA_names = data_MSOA[[\"MSOA11NM\"]].reset_index()\n",
    "    final_dfm.reset_index(inplace=True)\n",
    "    final_dfm[\"Group\"] = final_dfm.Group.astype(int)\n",
    "    save_df = pd.merge(MSOA_names,final_dfm, right_on=\"Group\", left_on=\"index\")\n",
    "    save_df.drop([\"index\",\"Group\"],axis=1, inplace=True)\n",
    "    save_df[\"Month\"] = date[-1]\n",
    "    \n",
    "    ## Save to pickle \n",
    "    filename = \"DataFrisk\" + date[-1] + \".p\"\n",
    "    #pickle.dump(save_df, open(filename, \"wb\"))\n",
    "    return save_df\n",
    "##--------------------------------------------------------------------------##\n",
    "\n",
    "## Convert polygons in geometry to coordinate lists\n",
    "save_list, index_list = [], []\n",
    "for index, item in enumerate(data_MSOA.geometry):\n",
    "    res = get_coords(item)\n",
    "    save_list.extend(res)\n",
    "    ## Keep index for multypoligons\n",
    "    index_list.extend([str(index) + \"-\" + str(x) for x in range(len(res))])\n",
    "\n",
    "dates = [\"2020-0\" + str(d) for d in range(1,10)]\n",
    "final_list = [frisk_to_pandas(x,save_list,index_list) for x in dates]\n",
    "all_frisk = pd.concat(final_list)\n",
    "pickle.dump(save_df, open(\"AllFrisk.p\", \"wb\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
